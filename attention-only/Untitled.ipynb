{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences():\n",
    "    s = []\n",
    "    for file in list(glob.glob('../data/rev-split/*.new')):                                       \n",
    "         with open(file) as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    line = line.strip()\n",
    "                    s.append(line)\n",
    "    s_train, s_test= train_test_split(s, test_size=0.33, random_state=42)\n",
    "    return s_train, s_test\n",
    "s_train, s_test = get_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59436"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import collections\n",
    "import itertools\n",
    "import typing\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformer\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'int: The total number of training epochs.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================================================================================================================== #\n",
    "#  C O N S T A N T S                                                                                                   #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "Token = collections.namedtuple(\"Token\", [\"index\", \"word\"])\n",
    "\"\"\"This is used to store index-word pairs.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "#  PARALLEL DATA  ######################################################################################################\n",
    "\n",
    "DATA_GERMAN = s_train\n",
    "DATA_ENGLISH = s_train\n",
    "\n",
    "DATA_GERMAN2 = s_test\n",
    "\n",
    "#  SPECIAL TOKENS  #####################################################################################################\n",
    "\n",
    "SOS = Token(0, \"<sos>\")\n",
    "\"\"\"str: The start-of-sequence token.\"\"\"\n",
    "\n",
    "EOS = Token(1, \"<eos>\")\n",
    "\"\"\"str: The end-of-sequence token.\"\"\"\n",
    "\n",
    "PAD = Token(2, \"<pad>\")\n",
    "\"\"\"str: The padding token.\"\"\"\n",
    "\n",
    "\n",
    "#  MODEL CONFIG  #######################################################################################################\n",
    "\n",
    "EMBEDDING_SIZE = 300\n",
    "\"\"\"int: The used embedding size.\"\"\"\n",
    "\n",
    "GPU = True  # <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< SET THIS TO True, IF YOU ARE USING A MACHINE WITH A GPU!\n",
    "\"\"\"bool: Indicates whether to make use of a GPU.\"\"\"\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "\"\"\"int: The total number of training epochs.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model: transformer.Transformer, input_seq: torch.LongTensor, target_seq: torch.LongTensor) -> None:\n",
    "    \"\"\"Evaluates the the provided model on the given data, and prints the probabilities of the desired translations.\n",
    "    \n",
    "    Args:\n",
    "        model (:class:`transformer.Transformer`): The model to evaluate.\n",
    "        input_seq (torch.LongTensor): The input sequences, as (batch-size x max-input-seq-len) tensor.\n",
    "        target_seq (torch.LongTensor): The target sequences, as (batch-size x max-target-seq-len) tensor.\n",
    "    \"\"\"\n",
    "    probs = transformer.eval_probability(model, input_seq, target_seq, pad_index=PAD.index).detach().numpy().tolist()\n",
    "    \n",
    "    print(\"sample       \" + (\"{}         \" * len(probs)).format(*range(len(probs))))\n",
    "    print(\"probability  \" + (\"{:.6f}  \" * len(probs)).format(*probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_vocab() -> typing.Tuple[typing.List[str], typing.Dict[str, int]]:\n",
    "    \"\"\"Determines the vocabulary, and provides mappings from indices to words and vice versa.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A pair of mappings, index-to-word and word-to-index.\n",
    "    \"\"\"\n",
    "    # gather all (lower-cased) words that appear in the data\n",
    "    all_words = set()\n",
    "    for sentence in itertools.chain(DATA_GERMAN, DATA_ENGLISH):\n",
    "        all_words.update(word.lower() for word in sentence.split(\" \"))\n",
    "    \n",
    "    # create mapping from index to word\n",
    "    idx_to_word = [SOS.word, EOS.word, PAD.word] + list(sorted(all_words))\n",
    "    \n",
    "    # create mapping from word to index\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(idx_to_word)}\n",
    "    \n",
    "    return idx_to_word, word_to_idx\n",
    "\n",
    "def fetch_vocab2() -> typing.Tuple[typing.List[str], typing.Dict[str, int]]:\n",
    "    \"\"\"Determines the vocabulary, and provides mappings from indices to words and vice versa.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A pair of mappings, index-to-word and word-to-index.\n",
    "    \"\"\"\n",
    "    # gather all (lower-cased) words that appear in the data\n",
    "    all_words = set()\n",
    "    for sentence in itertools.chain(DATA_GERMAN2, DATA_ENGLISH):\n",
    "        all_words.update(word.lower() for word in sentence.split(\" \"))\n",
    "    \n",
    "    # create mapping from index to word\n",
    "    idx_to_word = [SOS.word, EOS.word, PAD.word] + list(sorted(all_words))\n",
    "    \n",
    "    # create mapping from word to index\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(idx_to_word)}\n",
    "    \n",
    "    return idx_to_word, word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data2(word_to_idx: typing.Dict[str, int]) -> typing.Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "    \"\"\"Prepares the data as PyTorch ``LongTensor``s.\n",
    "    \n",
    "    Args:\n",
    "        word_to_idx (dict[str, int]): A dictionary that maps words to indices in the vocabulary.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A pair of ``LongTensor``s, the first representing the input and the second the target sequence.\n",
    "    \"\"\"\n",
    "    # break sentences into word tokens\n",
    "    german = []\n",
    "    for sentence in DATA_GERMAN2:\n",
    "        german.append([SOS.word] + sentence.split(\" \") + [EOS.word])\n",
    "    english = []\n",
    "    for sentence in DATA_ENGLISH:\n",
    "        english.append([SOS.word] + sentence.split(\" \") + [EOS.word])\n",
    "    \n",
    "    # pad all sentences to equal length\n",
    "    len_german = max(len(sentence) for sentence in german)\n",
    "    for sentence in german:\n",
    "        sentence.extend([PAD.word] * (len_german - len(sentence)))\n",
    "    len_english = max(len(sentence) for sentence in english)\n",
    "    for sentence in english:\n",
    "        sentence.extend([PAD.word] * (len_english - len(sentence)))\n",
    "    \n",
    "    # map words to indices in the vocabulary\n",
    "    german = [[word_to_idx[word.lower()] for word in sentence] for sentence in german]\n",
    "    english = [[word_to_idx[word.lower()] for word in sentence] for sentence in english]\n",
    "    \n",
    "    # create according LongTensors\n",
    "    german = torch.LongTensor(german)\n",
    "    english = torch.LongTensor(english)\n",
    "    \n",
    "    return german, english\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(word_to_idx: typing.Dict[str, int]) -> typing.Tuple[torch.LongTensor, torch.LongTensor]:\n",
    "    \"\"\"Prepares the data as PyTorch ``LongTensor``s.\n",
    "    \n",
    "    Args:\n",
    "        word_to_idx (dict[str, int]): A dictionary that maps words to indices in the vocabulary.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A pair of ``LongTensor``s, the first representing the input and the second the target sequence.\n",
    "    \"\"\"\n",
    "    # break sentences into word tokens\n",
    "    german = []\n",
    "    for sentence in DATA_GERMAN:\n",
    "        german.append([SOS.word] + sentence.split(\" \") + [EOS.word])\n",
    "    english = []\n",
    "    for sentence in DATA_ENGLISH:\n",
    "        english.append([SOS.word] + sentence.split(\" \") + [EOS.word])\n",
    "    \n",
    "    # pad all sentences to equal length\n",
    "    len_german = max(len(sentence) for sentence in german)\n",
    "    for sentence in german:\n",
    "        sentence.extend([PAD.word] * (len_german - len(sentence)))\n",
    "    len_english = max(len(sentence) for sentence in english)\n",
    "    for sentence in english:\n",
    "        sentence.extend([PAD.word] * (len_english - len(sentence)))\n",
    "    \n",
    "    # map words to indices in the vocabulary\n",
    "    german = [[word_to_idx[word.lower()] for word in sentence] for sentence in german]\n",
    "    english = [[word_to_idx[word.lower()] for word in sentence] for sentence in english]\n",
    "    \n",
    "    # create according LongTensors\n",
    "    german = torch.LongTensor(german)\n",
    "    english = torch.LongTensor(english)\n",
    "    \n",
    "    return german, english\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Probabilities of Translations:\n",
      "--------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 158GB. Buy new RAM! at /pytorch/aten/src/TH/THGeneral.cpp:204",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-91961a62fc0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-91961a62fc0a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial Probabilities of Translations:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--------------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-174b565f9097>\u001b[0m in \u001b[0;36meval_model\u001b[0;34m(model, input_seq, target_seq)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtarget_seq\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msize\u001b[0m \u001b[0mx\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \"\"\"\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPAD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample       \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"{}         \"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/transformer/transformer_tools.py\u001b[0m in \u001b[0;36meval_probability\u001b[0;34m(model, input_seq, target_seq, pad_index)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# run the model to compute the needed probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# determine the lengths of the target sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/transformer/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, target)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# embed the provided input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0minput_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_word_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_positional_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;31m# project input to the needed size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    108\u001b[0m         return F.embedding(\n\u001b[1;32m    109\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 158GB. Buy new RAM! at /pytorch/aten/src/TH/THGeneral.cpp:204"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "#  H E L P E R  F U N C T I O N S                                                                                      #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==================================================================================================================== #\n",
    "#  M A I N                                                                                                             #\n",
    "# ==================================================================================================================== #\n",
    "\n",
    "\n",
    "def main():\n",
    "    # fetch vocabulary + prepare data\n",
    "    idx_to_word, word_to_idx = fetch_vocab()\n",
    "    input_seq, target_seq = prepare_data(word_to_idx)\n",
    "    \n",
    "    # create embeddings to use\n",
    "    emb = nn.Embedding(len(idx_to_word), EMBEDDING_SIZE)\n",
    "    emb.reset_parameters()\n",
    "    return\n",
    "    # create transformer model\n",
    "    model = transformer.Transformer(\n",
    "            emb,\n",
    "            PAD.index,\n",
    "            emb.num_embeddings,\n",
    "            max_seq_len=max(input_seq.size(1), target_seq.size(1))\n",
    "    )\n",
    "\n",
    "    # create an optimizer for training the model + a X-entropy loss\n",
    "    optimizer = optim.Adam((param for param in model.parameters() if param.requires_grad), lr=0.0001)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"Initial Probabilities of Translations:\")\n",
    "    print(\"--------------------------------------\")\n",
    "    eval_model(model, input_seq, target_seq)\n",
    "    print()\n",
    "    \n",
    "    # move model + data on the GPU (if possible)\n",
    "    if GPU:\n",
    "        model.cuda()\n",
    "        input_seq = input_seq.cuda()\n",
    "        target_seq = target_seq.cuda()\n",
    "\n",
    "    # train the model\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"training epoch {}...\".format(epoch + 1), end=\" \")\n",
    "    \n",
    "        predictions = model(input_seq, target_seq)\n",
    "        optimizer.zero_grad()\n",
    "        current_loss = loss(\n",
    "                predictions.view(predictions.size(0) * predictions.size(1), predictions.size(2)),\n",
    "                target_seq.view(-1)\n",
    "        )\n",
    "        current_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        print(\"OK (loss: {:.6f})\".format(current_loss.item()))\n",
    "    \n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    print()\n",
    "    print(\"Final Probabilities of Translations:\")\n",
    "    print(\"------------------------------------\")\n",
    "    eval_model(model, input_seq, target_seq)\n",
    "    \n",
    "    # randomly sample outputs from the input sequences based on the probabilities computed by the trained model\n",
    "    sampled_output = transformer.sample_output(model, input_seq, EOS.index, PAD.index, target_seq.size(1))\n",
    "    idx_to_word, word_to_idx = fetch_vocab2()\n",
    "    input_seq, target_seq = prepare_data2(word_to_idx)\n",
    "\n",
    "    print()\n",
    "    print(\"Sampled Outputs:\")\n",
    "    print(\"----------------\")\n",
    "    for sample_idx in range(input_seq.size(0)):\n",
    "        for token_idx in range(input_seq.size(1)):\n",
    "            print(idx_to_word[input_seq[sample_idx, token_idx].item()], end=\" \")\n",
    "        print(\" => \", end=\" \")\n",
    "        for token_idx in range(sampled_output.size(1)):\n",
    "            print(idx_to_word[sampled_output[sample_idx, token_idx].item()], end=\" \")\n",
    "        print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
